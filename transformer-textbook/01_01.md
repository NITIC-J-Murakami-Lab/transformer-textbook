# Transformer 導入

世界中の人が chatGPT や Gemini などの生成 AI サービスを日常的に利用しているが、現在のほとんどの大規模言語モデル LLM は Transformer というアーキテクチャをベースとしている。
ここでは従来の系列モデルと比べてどのような特徴を持つのかを整理する。

## 1. 系列データとその難しさ

自然言語や音声、各種ログなど、時系列で並ぶデータはすべて系列とみなせる。系列では、要素の順番が意味を形成するため、モデルはこの順序情報と要素間のつながりを適切に扱う必要がある。

中でも重要になるのが、離れた位置どうしの関係、つまり長距離依存である。  
例を挙げると、文の前半にある主語と、後半の述語が対応しているようなケースである。こうした長距離依存が捉えられないと、多くの自然言語処理タスクで正しい判断ができない。

## 2. 従来手法の特徴と限界

Transformer が登場する前、系列モデリングでは再帰型ネットワーク（RNN／LSTM／GRU）や畳み込みネットワーク（CNN 系）が主流だった。それぞれ一定の効果はあったが、根本的な制約も存在していた。（RNN, LSTM は四年前期の授業で扱う）

### 2.1 再帰型ネットワーク（RNN 系）

- 系列を一つずつ順に読み込む
- 過去の計算に依存するため並列化しづらい
- 長い系列では勾配が弱まり、遠くの情報が伝わりにくい
- LSTM や GRU によって改善されたものの、構造的な性質はそのまま残った

順番に処理する特性が、計算効率と長距離依存のどちらにも悪い影響を与えていた。

### 2.2 畳み込み系モデル

- 近い位置の情報をまとめることには優れる
- ただし遠い位置の情報まで届かせるには層を多く積む必要がある
- → 計算量が増し、深いネットワークになる

RNN も CNN も、系列の遠く離れた要素どうしをうまく扱うという課題を完全には解決できていなかった。

## 3. Transformer の考え方

Transformer は、この従来手法の限界を踏まえて設計されたモデルである。  
中心となるのは、再帰や畳み込みに頼らず、注意機構（attention）だけで系列全体を処理するという発想である。

### 3.1 全要素を一度に扱う

Transformer は系列を逐次的に入力せず、全体を同時に受け取る。  
→ これによって GPU 上での並列化が容易になり、学習速度が大きく向上する。（デカデカメリット）

### 3.2 注意を使った柔軟な依存関係の表現

自己注意では、任意の位置の要素が他のどの位置にも注意を向けられる。  
距離によらず関連性を表現できるため、長距離依存の扱いが構造的に簡単になる。

→ 従来のように深い再帰や多数の畳み込み層を通さなくても、遠い情報を直接参照できる

また、再帰型 Seq2Seq モデルで用いられていた encoder-decoder の考え方を残しつつ、内部の処理をすべて注意機構で統一したことにより構造が明快になり、学習の安定性も高まる。

## 4. Transformer と従来手法

ここでは、細かな計算ではなく構造上の違いが生むメリットに着目する。

### 4.1 情報の伝わり方が距離に依存しない

RNN は、距離が d ある依存関係を捉えるために d 回の伝播が必要になる。  
CNN は層を積むことが必要になる。

一方、Transformer の注意機構では、どの位置からでも直接別の位置にアクセスできるので情報の通り道が最短化され、長距離依存を扱う上で大きな利点となる。

### 4.2 学習の並列化が容易

- RNN: 構造上逐次処理が避けられず、計算が遅い
- CNN: 一部並列化可能だが、深さに依存して効率が下がる
- Transformer: 全系列を同時に扱うため最も並列計算に向く

### 4.3 モデルの拡張性が高い

注意ヘッドや層を増やすことで、性能を比較的容易に向上させられる。  
この性質が、後続の BERT や GPT、さらには Vision Transformer などの大型モデルにつながっている。

## 5. Transformer による応用範囲の拡大

Transformer の設計は自然言語に限らず、画像、音声、マルチモーダルなど多くの分野に応用されている。  
BERT による特徴抽出、GPT による生成、ViT による画像分類、広範なタスクで基盤技術となっている。
（もともとは翻訳タスクのために研究さていた）

## 6. まとめ

- 系列データでは長距離依存の扱いが難題だった
- RNN と CNN は構造的に長距離依存が苦手
- Transformer は注意機構により距離に依存しない相互作用を可能にした
- 並列計算しやすく、大規模学習に適している
