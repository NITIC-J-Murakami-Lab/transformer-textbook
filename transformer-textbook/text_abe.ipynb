{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Z4C1OWLiljUS",
        "kBouI1HKn2IQ",
        "09g6AWuvri_t",
        "2jJhGlnSwZZe"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##　入力されたデータはどのように処理されるか\n"
      ],
      "metadata": {
        "id": "uUmC-IrniWG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.単語の数値化"
      ],
      "metadata": {
        "id": "Z4C1OWLiljUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. まず、入力されたデータをコンピュータはそのままの文字を理解できないため、まずは計算可能な数値（ベクトル）に変換する。\n",
        "2. 入力された文章を単語ごとに区切る。\n",
        "3. それぞれの単語を、辞書に基づいて固有のID番号に変換する。\n",
        "4. ID番号を、意味を持つ密なベクトル（数百〜数千次元の数字の列）にする。これにより、「王」と「男」のような意味の近い単語が、数学的にも近い距離に配置され分別されやすくなる。"
      ],
      "metadata": {
        "id": "CsrN38Q-idDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###　2.Positional Encoding"
      ],
      "metadata": {
        "id": "kBouI1HKn2IQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここがRNNと大きく違う点である。 トランスフォーマーは文章を頭から順番に読むのではなく、一気に並列で処理を行っている。しかし、そのままだと「AがBに殴った」と「BがAを殴った」の語順の違いがわからなくなってしまう。<br>\n",
        "そこで各単語のベクトルにこの単語は何番目にあるかを示す位置情報ベクトル（Positional Encoding）を足し合わせる。<br>\n",
        "それにより先述した語順の違いが判らなくなるという致命的な問題をなくすることができる。"
      ],
      "metadata": {
        "id": "hptviUnln6uX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Self-Attention"
      ],
      "metadata": {
        "id": "09g6AWuvri_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "self-attensionでは、入力された単語同士がどれくらい関係しているかを計算し、文脈を理解するのに使われている。<br>\n",
        "仕組みとしては、ある単語を処理する際、文中の他のすべての単語との関連度を計算する。その結果、単語単体の意味だけでなく、文脈を含んだ意味を持つベクトルに変換される。(単語単体だと意味が一意に定まらない単語が一意に定まる)<br>この情報は、Feed-Forward層に渡される。"
      ],
      "metadata": {
        "id": "q4cPLfxWr1no"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attension"
      ],
      "metadata": {
        "id": "2jJhGlnSwZZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Head Attensionでは入力された文全体を見て、各単語が他の単語とどの程度関連しているかを計算する。 普通のAttensionと比べてMulti-Head Attensionは、単一のAttensionを並行して実行し、それぞれの結果を統合することで、モデルの表現力を劇的に高めている。"
      ],
      "metadata": {
        "id": "UZymJnRyweEj"
      }
    }
  ]
}