# アーキテクチャ解説
## 入力されたデータはどのように処理されるか
### 1.単語の数値化
1. まず、入力されたデータをコンピュータはそのままの文字を理解できないため、まずは計算可能な数値（ベクトル）に変換する。
2. 入力された文章を単語ごとに区切る。
3. それぞれの単語を、辞書に基づいて固有のID番号に変換する。
4. ID番号を、意味を持つ密なベクトル（数百〜数千次元の数字の列）にする。これにより、「王」と「男」のような意味の近い単語が、数学的にも近い距離に配置され分別されやすくなる。<br>
### 2.Positional Encoding
ここがRNNと大きく違う点である。 トランスフォーマーは文章を頭から順番に読むのではなく、一気に並列で処理を行っている。しかし、そのままだと「AがBに殴った」と「BがAを殴った」の語順の違いがわからなくなってしまう。<br>
そこで各単語のベクトルにこの単語は何番目にあるかを示す位置情報ベクトル（Positional Encoding）を足し合わせる。<br>
それにより先述した語順の違いが判らなくなるという致命的な問題をなくすることができる。
### 3.Self-Attention
self-attensionでは、入力された単語同士がどれくらい関係しているかを計算し、文脈を理解するのに使われている。<br>
仕組みとしては、ある単語を処理する際、文中の他のすべての単語との関連度を計算する。その結果、単語単体の意味だけでなく、文脈を含んだ意味を持つベクトルに変換される。(単語単体だと意味が一意に定まらない単語が一意に定まる)<br>この情報は、Feed-Forward層に渡される。
## Multi-Head Attension
### Multi-Head Attensionの概要<br>

Multi-Head Attensionでは入力された文全体を見て、各単語が他の単語とどの程度関連しているかを計算する。 普通のAttensionと比べてMulti-Head Attensionは、単一のAttensionを並行して実行し、それぞれの結果を統合することで、モデルの表現力を劇的に高めている。

### 技術的な仕組み<br>
### 1.入力データの準備
まず、入力された単語ベクトルから、以下の3つのベクトルを作成する。
- Query (Q): 検索クエリ（何を探したいか）
- Key (K): 検索対象のインデックス（どこに何があるか）
- Value (V): 抽出する中身（実際の情報）
### 2.ヘッドへの分割
Multi-Head Attensionでは、ここが重要である。元の大きなベクトルをそのまま使うのではなく、ヘッドの数だけ分割する。 例えば、元の次元数が512でヘッドが8つなら、各ヘッドは64次元の小さな空間で計算を行う。これにより、各ヘッドが「異なる部分空間」を学習できるようになる。
### 3.並列計算
分割された各ヘッドで、独立してAttentionを計算する。
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
($d_k$ はベクトルの次元数。これで割ることで勾配消失を防ぐ。)
### 4.結合と仕上げ
各ヘッドから出力された結果を連結して元のサイズに戻し、最後にもう一度線形変換（重み行列 $W^O$ を掛ける）を行って、最終的な出力とします。
### Multi-Head Attentionのメリット
- CNNやRNNに比べ、離れた位置にある単語同士の関係性を捉えるのが得意である。
- ヘッドごとに次元を小さくして並列計算するため、トータルの計算量はSingle-Headの場合と大きく変わらない。
### まとめ
Multi-Head Attentionは、Transformerアーキテクチャの中核を担うコンポーネントであり、従来のAttentionを並列化して拡張したものである。複数のヘッドが異なる部分空間の情報に同時に着目することで、複雑な文脈や長距離の依存関係を捉えるモデルの表現力を飛躍的に高めている。
